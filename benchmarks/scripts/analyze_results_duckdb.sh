#!/bin/bash

# DuckDB Results Analysis Script for groxpi Benchmark Suite
# Analyzes CSV files generated by the benchmark suite using DuckDB SQL
# Usage: ./analyze_results_duckdb.sh [timestamp] [results_dir]

set -euo pipefail

# Configuration
TIMESTAMP="${1:-}"
RESULTS_DIR="${2:-$(dirname "$0")/../results}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Function to show usage
show_usage() {
    echo "DuckDB Results Analysis for groxpi Benchmark Suite"
    echo ""
    echo "Usage: $0 [timestamp] [results_dir]"
    echo ""
    echo "Parameters:"
    echo "  timestamp   - Specific timestamp to analyze (e.g., 20240101_120000)"
    echo "  results_dir - Results directory (default: ../results)"
    echo ""
    echo "Examples:"
    echo "  $0                                    # Analyze latest results"
    echo "  $0 20240101_120000                    # Analyze specific timestamp"
    echo "  $0 20240101_120000 /path/to/results   # Custom results directory"
    echo ""
    echo "Requirements:"
    echo "  - DuckDB CLI installed (brew install duckdb)"
}

# Function to check dependencies
check_dependencies() {
    if ! command -v duckdb >/dev/null 2>&1; then
        log_error "DuckDB is not installed. Please install it first:"
        echo "  macOS: brew install duckdb"
        echo "  Ubuntu/Debian: wget https://github.com/duckdb/duckdb/releases/latest/download/duckdb_cli-linux-amd64.zip"
        echo "  See: https://duckdb.org/docs/installation/"
        exit 1
    fi

    log_info "Using DuckDB: $(which duckdb)"
    duckdb --version
}

# Function to find latest CSV files
find_csv_files() {
    local pattern_suffix="$1"
    local files

    if [ -n "$TIMESTAMP" ]; then
        files=($(find "$RESULTS_DIR" -name "*${TIMESTAMP}*${pattern_suffix}" -type f 2>/dev/null | sort))
    else
        files=($(find "$RESULTS_DIR" -name "*${pattern_suffix}" -type f 2>/dev/null | sort -t'_' -k2 -r))
    fi

    echo "${files[@]}"
}

# Function to run DuckDB query and format output
run_query() {
    local query="$1"
    local title="$2"
    local output_file="${3:-}"

    echo ""
    echo "=========================================="
    echo "=== $title ==="
    echo "=========================================="
    echo ""

    if [ -n "$output_file" ]; then
        echo "Query: $query" > "$output_file"
        echo "" >> "$output_file"
        duckdb -c "$query" | tee -a "$output_file"
    else
        duckdb -c "$query"
    fi
}

# Function to analyze resource usage
analyze_resources() {
    local resource_files=($(find_csv_files "resources*.csv"))

    if [ ${#resource_files[@]} -eq 0 ]; then
        log_warning "No resource CSV files found"
        return
    fi

    local resource_file="${resource_files[0]}"
    log_info "Analyzing resource data: $(basename "$resource_file")"

    # Resource usage summary
    run_query "
    SELECT
        container as service,
        ROUND(AVG(cpu_percent), 2) as avg_cpu_percent,
        ROUND(MAX(cpu_percent), 2) as max_cpu_percent,
        ROUND(AVG(memory_used_mb), 1) as avg_memory_mb,
        ROUND(MAX(memory_used_mb), 1) as max_memory_mb,
        COUNT(*) as data_points,
        DATE_DIFF('minute', MIN(timestamp::timestamp), MAX(timestamp::timestamp)) as duration_minutes
    FROM read_csv('$resource_file', header=true)
    WHERE cpu_percent IS NOT NULL
    GROUP BY container
    ORDER BY container;
    " "Resource Usage Summary"

    # Resource usage over time (peak periods)
    run_query "
    SELECT
        container as service,
        timestamp,
        cpu_percent,
        memory_used_mb,
        CASE
            WHEN cpu_percent > 50 THEN 'High CPU'
            WHEN memory_used_mb > 1000 THEN 'High Memory'
            ELSE 'Normal'
        END as load_level
    FROM read_csv('$resource_file', header=true)
    WHERE cpu_percent IS NOT NULL
    AND (cpu_percent > 30 OR memory_used_mb > 500)
    ORDER BY timestamp DESC
    LIMIT 20;
    " "Peak Resource Usage Periods"
}

# Function to analyze API performance
analyze_api_performance() {
    local wrk_files=($(find_csv_files "wrk-summary*.csv"))

    if [ ${#wrk_files[@]} -eq 0 ]; then
        log_warning "No WRK summary CSV files found"
        return
    fi

    local wrk_file="${wrk_files[0]}"
    log_info "Analyzing API performance data: $(basename "$wrk_file")"

    # Load test performance comparison
    run_query "
    SELECT
        service,
        test_name,
        endpoint,
        ROUND(AVG(requests_per_sec), 1) as avg_rps,
        ROUND(MAX(requests_per_sec), 1) as max_rps,
        ROUND(AVG(avg_latency_ms), 2) as avg_latency_ms,
        ROUND(AVG(p99_latency_ms), 2) as p99_latency_ms,
        COUNT(*) as test_runs
    FROM read_csv('$wrk_file', header=true)
    WHERE test_type = 'load' AND requests_per_sec IS NOT NULL
    GROUP BY service, test_name, endpoint
    ORDER BY service, avg_rps DESC;
    " "API Load Test Performance"

    # Single request response times
    run_query "
    SELECT
        service,
        test_name,
        endpoint,
        ROUND(AVG(response_time_single_sec), 4) as avg_response_time_sec,
        ROUND(MIN(response_time_single_sec), 4) as min_response_time_sec,
        ROUND(MAX(response_time_single_sec), 4) as max_response_time_sec,
        COUNT(*) as test_runs
    FROM read_csv('$wrk_file', header=true)
    WHERE test_type = 'single' AND response_time_single_sec IS NOT NULL
    GROUP BY service, test_name, endpoint
    ORDER BY service, avg_response_time_sec;
    " "Single Request Response Times"

    # Performance comparison (side by side)
    run_query "
    WITH groxpi_perf AS (
        SELECT
            test_name,
            endpoint,
            ROUND(AVG(requests_per_sec), 1) as groxpi_rps,
            ROUND(AVG(avg_latency_ms), 2) as groxpi_latency_ms
        FROM read_csv('$wrk_file', header=true)
        WHERE service = 'groxpi' AND test_type = 'load' AND requests_per_sec IS NOT NULL
        GROUP BY test_name, endpoint
    ),
    proxpi_perf AS (
        SELECT
            test_name,
            endpoint,
            ROUND(AVG(requests_per_sec), 1) as proxpi_rps,
            ROUND(AVG(avg_latency_ms), 2) as proxpi_latency_ms
        FROM read_csv('$wrk_file', header=true)
        WHERE service = 'proxpi' AND test_type = 'load' AND requests_per_sec IS NOT NULL
        GROUP BY test_name, endpoint
    )
    SELECT
        g.test_name,
        g.endpoint,
        g.groxpi_rps,
        p.proxpi_rps,
        ROUND(g.groxpi_rps / NULLIF(p.proxpi_rps, 0), 2) as rps_speedup_ratio,
        g.groxpi_latency_ms,
        p.proxpi_latency_ms,
        ROUND(p.proxpi_latency_ms / NULLIF(g.groxpi_latency_ms, 0), 2) as latency_improvement_ratio
    FROM groxpi_perf g
    FULL OUTER JOIN proxpi_perf p ON g.test_name = p.test_name AND g.endpoint = p.endpoint
    ORDER BY rps_speedup_ratio DESC NULLS LAST;
    " "Performance Comparison (groxpi vs proxpi)"
}

# Function to analyze installation performance
analyze_installation_performance() {
    local uv_files=($(find_csv_files "uv-summary*.csv"))

    if [ ${#uv_files[@]} -eq 0 ]; then
        log_warning "No UV summary CSV files found"
        return
    fi

    local uv_file="${uv_files[0]}"
    log_info "Analyzing installation performance data: $(basename "$uv_file")"

    # Installation performance by package and cache mode
    run_query "
    SELECT
        service,
        package,
        cache_mode,
        ROUND(AVG(install_time_seconds), 2) as avg_install_time_sec,
        ROUND(MIN(install_time_seconds), 2) as min_install_time_sec,
        ROUND(MAX(install_time_seconds), 2) as max_install_time_sec,
        COUNT(*) as successful_installs,
        ROUND(AVG(installed_size_mb), 1) as avg_size_mb
    FROM read_csv('$uv_file', header=true)
    WHERE install_success = true AND test_type = 'individual'
    GROUP BY service, package, cache_mode
    ORDER BY service, package, cache_mode;
    " "Package Installation Performance"

    # Cache effectiveness (cold vs warm)
    run_query "
    WITH cache_comparison AS (
        SELECT
            service,
            package,
            cache_mode,
            AVG(install_time_seconds) as avg_time
        FROM read_csv('$uv_file', header=true)
        WHERE install_success = true AND test_type = 'individual'
        AND cache_mode IN ('cold', 'warm')
        GROUP BY service, package, cache_mode
    ),
    pivot_data AS (
        SELECT
            service,
            package,
            MAX(CASE WHEN cache_mode = 'cold' THEN avg_time END) as cold_time,
            MAX(CASE WHEN cache_mode = 'warm' THEN avg_time END) as warm_time
        FROM cache_comparison
        GROUP BY service, package
    )
    SELECT
        service,
        package,
        ROUND(cold_time, 2) as cold_cache_sec,
        ROUND(warm_time, 2) as warm_cache_sec,
        ROUND(cold_time - warm_time, 2) as time_saved_sec,
        ROUND(((cold_time - warm_time) / cold_time) * 100, 1) as cache_effectiveness_percent
    FROM pivot_data
    WHERE cold_time IS NOT NULL AND warm_time IS NOT NULL
    ORDER BY service, cache_effectiveness_percent DESC;
    " "Cache Effectiveness Analysis"

    # Package size analysis
    run_query "
    SELECT
        package,
        service,
        ROUND(AVG(installed_size_mb), 1) as avg_size_mb,
        ROUND(AVG(install_time_seconds), 2) as avg_install_time_sec,
        ROUND(AVG(dependency_count), 0) as avg_dependencies,
        ROUND(AVG(install_time_seconds) / NULLIF(AVG(installed_size_mb), 0), 4) as sec_per_mb
    FROM read_csv('$uv_file', header=true)
    WHERE install_success = true
    AND installed_size_mb IS NOT NULL
    AND installed_size_mb > 0
    AND test_type = 'individual'
    GROUP BY package, service
    ORDER BY avg_size_mb DESC;
    " "Package Size vs Installation Time"

    # Batch installation efficiency
    run_query "
    SELECT
        service,
        test_name,
        package as batch_type,
        ROUND(install_time_seconds, 2) as batch_install_time_sec,
        ROUND(installed_size_mb, 1) as total_size_mb,
        dependency_count as total_dependencies,
        ROUND(installed_size_mb / install_time_seconds, 2) as mb_per_second
    FROM read_csv('$uv_file', header=true)
    WHERE test_type = 'batch' AND install_success = true
    ORDER BY service, mb_per_second DESC;
    " "Batch Installation Efficiency"

    # Installation success rates
    run_query "
    SELECT
        service,
        cache_mode,
        test_type,
        COUNT(*) as total_attempts,
        SUM(CASE WHEN install_success THEN 1 ELSE 0 END) as successful_installs,
        ROUND((SUM(CASE WHEN install_success THEN 1 ELSE 0 END) * 100.0 / COUNT(*)), 1) as success_rate_percent
    FROM read_csv('$uv_file', header=true)
    GROUP BY service, cache_mode, test_type
    ORDER BY service, cache_mode, test_type;
    " "Installation Success Rates"
}

# Function to generate executive summary
generate_executive_summary() {
    local wrk_files=($(find_csv_files "wrk-summary*.csv"))
    local uv_files=($(find_csv_files "uv-summary*.csv"))
    local resource_files=($(find_csv_files "resources*.csv"))

    echo ""
    echo "=========================================="
    echo "=== EXECUTIVE SUMMARY ==="
    echo "=========================================="
    echo ""

    if [ ${#wrk_files[@]} -gt 0 ]; then
        local wrk_file="${wrk_files[0]}"
        echo "📊 API Performance Highlights:"
        duckdb -c "
        WITH perf_summary AS (
            SELECT
                service,
                AVG(requests_per_sec) as avg_rps
            FROM read_csv('$wrk_file', header=true)
            WHERE test_type = 'load' AND requests_per_sec IS NOT NULL
            GROUP BY service
        )
        SELECT
            '🚀 ' || service || ' average: ' || ROUND(avg_rps, 0) || ' RPS' as summary
        FROM perf_summary
        ORDER BY avg_rps DESC;
        "

        duckdb -c "
        WITH speedup AS (
            SELECT
                ROUND(MAX(requests_per_sec) / MIN(requests_per_sec), 1) as speedup_ratio
            FROM read_csv('$wrk_file', header=true)
            WHERE test_type = 'load' AND requests_per_sec IS NOT NULL
        )
        SELECT '⚡ Performance advantage: ' || speedup_ratio || 'x faster' as summary
        FROM speedup;
        "
    fi

    if [ ${#uv_files[@]} -gt 0 ]; then
        local uv_file="${uv_files[0]}"
        echo ""
        echo "📦 Installation Performance Highlights:"
        duckdb -c "
        WITH install_summary AS (
            SELECT
                service,
                ROUND(AVG(install_time_seconds), 1) as avg_time
            FROM read_csv('$uv_file', header=true)
            WHERE install_success = true AND test_type = 'individual' AND cache_mode = 'cold'
            GROUP BY service
        )
        SELECT
            '📦 ' || service || ' cold cache avg: ' || avg_time || 's' as summary
        FROM install_summary
        ORDER BY avg_time;
        "

        duckdb -c "
        WITH success_rates AS (
            SELECT
                service,
                ROUND((SUM(CASE WHEN install_success THEN 1 ELSE 0 END) * 100.0 / COUNT(*)), 1) as success_rate
            FROM read_csv('$uv_file', header=true)
            GROUP BY service
        )
        SELECT '✅ ' || service || ' success rate: ' || success_rate || '%' as summary
        FROM success_rates
        ORDER BY success_rate DESC;
        "
    fi

    if [ ${#resource_files[@]} -gt 0 ]; then
        local resource_file="${resource_files[0]}"
        echo ""
        echo "💻 Resource Usage Highlights:"
        duckdb -c "
        SELECT
            '🔥 ' || container || ' peak CPU: ' || ROUND(MAX(cpu_percent), 1) || '%' as summary
        FROM read_csv('$resource_file', header=true)
        WHERE cpu_percent IS NOT NULL
        GROUP BY container
        ORDER BY MAX(cpu_percent) DESC;
        "

        duckdb -c "
        SELECT
            '🧠 ' || container || ' peak memory: ' || ROUND(MAX(memory_used_mb), 0) || 'MB' as summary
        FROM read_csv('$resource_file', header=true)
        WHERE memory_used_mb IS NOT NULL
        GROUP BY container
        ORDER BY MAX(memory_used_mb) DESC;
        "
    fi
}

# Function to export analysis to markdown report
export_markdown_report() {
    local output_file="$RESULTS_DIR/duckdb-analysis-${TIMESTAMP:-$(date +%Y%m%d_%H%M%S)}.md"

    log_info "Generating markdown analysis report: $output_file"

    {
        echo "# groxpi vs proxpi Benchmark Analysis"
        echo ""
        echo "**Generated:** $(date '+%Y-%m-%d %H:%M:%S')"
        echo "**Timestamp:** ${TIMESTAMP:-latest}"
        echo "**Analysis Tool:** DuckDB $(duckdb --version | head -1)"
        echo ""

        echo "## Executive Summary"
        generate_executive_summary 2>/dev/null

        echo ""
        echo "## Detailed Analysis"
        echo ""
        echo "Run the following command for interactive analysis:"
        echo "\`\`\`bash"
        echo "./analyze_results_duckdb.sh ${TIMESTAMP:-}"
        echo "\`\`\`"

        echo ""
        echo "## Raw Data Files"
        echo ""
        local all_files=($(find "$RESULTS_DIR" -name "*.csv" -type f | sort))
        for file in "${all_files[@]}"; do
            echo "- \`$(basename "$file")\`"
        done

        echo ""
        echo "## DuckDB Queries"
        echo ""
        echo "You can run custom analysis using DuckDB CLI:"
        echo "\`\`\`sql"
        echo "-- Example: Custom API performance query"
        echo "SELECT service, AVG(requests_per_sec) as avg_rps"
        echo "FROM read_csv_auto('$RESULTS_DIR/wrk-summary-*.csv')"
        echo "WHERE test_type = 'load'"
        echo "GROUP BY service;"
        echo "\`\`\`"

    } > "$output_file"

    log_success "Markdown report saved: $output_file"
}

# Main function
main() {
    local start_time=$(date '+%Y-%m-%d %H:%M:%S')

    echo "=========================================="
    echo "=== DuckDB Benchmark Analysis ==="
    echo "=========================================="
    echo "Start time: $start_time"
    echo "Results directory: $RESULTS_DIR"
    echo "Timestamp filter: ${TIMESTAMP:-latest}"
    echo ""

    # Check if help is requested
    if [[ "${1:-}" == "-h" ]] || [[ "${1:-}" == "--help" ]]; then
        show_usage
        exit 0
    fi

    # Validate environment
    check_dependencies

    if [ ! -d "$RESULTS_DIR" ]; then
        log_error "Results directory does not exist: $RESULTS_DIR"
        exit 1
    fi

    # Run analysis
    analyze_resources
    analyze_api_performance
    analyze_installation_performance
    generate_executive_summary

    # Export markdown report
    export_markdown_report

    local end_time=$(date '+%Y-%m-%d %H:%M:%S')
    echo ""
    echo "=========================================="
    echo "=== Analysis Complete ==="
    echo "=========================================="
    echo "Start time: $start_time"
    echo "End time: $end_time"
    echo ""

    log_success "DuckDB analysis completed successfully"
    log_info "💡 Tip: You can run custom queries with: duckdb -c \"SELECT * FROM read_csv_auto('$RESULTS_DIR/wrk-summary-*.csv') LIMIT 5;\""
}

# Run main function
main "$@"